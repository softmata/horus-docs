---
title: "GPU Tensor Sharing"
description: "Zero-copy GPU memory sharing across processes using CUDA IPC for high-performance AI/ML robotics"
weight: 45
---

# GPU Tensor Sharing

HORUS provides native CUDA IPC (Inter-Process Communication) support for sharing GPU tensors between processes with zero-copy semantics. This enables high-performance AI/ML robotics pipelines where perception, planning, and control can share GPU data without expensive CPU roundtrips.

## Key Takeaways

After reading this guide, you will understand:
- How CUDA IPC enables zero-copy GPU memory sharing
- When to use GPU tensor sharing vs CPU tensors
- How to implement cross-process GPU pipelines
- **Tensor views and slices** for zero-copy reshaping and cropping
- **Multi-GPU P2P** for direct GPU-to-GPU transfers
- **CUDA streams** for async operations and overlapping transfers
- Performance characteristics and limitations

## Why GPU Tensor Sharing?

### The Problem

Traditional robotics frameworks copy data through CPU memory:

```
Process A (Perception)     Process B (Planning)
┌─────────────────┐       ┌─────────────────┐
│ GPU: Run YOLO   │       │ GPU: Path Plan  │
│       ↓         │       │       ↑         │
│ GPU → CPU copy  │──────→│ CPU → GPU copy  │
└─────────────────┘ IPC   └─────────────────┘
        ~2ms                      ~2ms
```

This adds **4ms+ latency** per frame for a 1080p image.

### The Solution

HORUS uses CUDA IPC to share GPU memory directly:

```
Process A (Perception)     Process B (Planning)
┌─────────────────┐       ┌─────────────────┐
│ GPU: Run YOLO   │       │ GPU: Path Plan  │
│       ↓         │       │       ↑         │
│ IPC Handle (64B)│──────→│ Open IPC Handle │
└─────────────────┘       └─────────────────┘
        ~1μs                      ~1μs
```

Only a 64-byte handle is transferred. Both processes access the **same physical GPU memory**.

## Enabling CUDA Support

### Build Configuration

There are three ways to enable CUDA support:

**Option 1: Command-line flag (recommended)**

```bash
horus run --enable cuda
```

**Option 2: Explicit in horus.yaml**

```yaml
enable:
  - cuda
```

Then run:
```bash
horus run
```

**Option 3: Manual Cargo.toml (for cargo projects)**

If using `cargo` directly instead of `horus run`:

```toml
[dependencies]
horus_core = { version = "0.1", features = ["cuda"] }
```

### Runtime Requirements

- NVIDIA GPU with compute capability 3.0+
- CUDA 11.0+ runtime installed
- Linux (CUDA IPC not supported on Windows/macOS)

Check availability at runtime:

```rust
use horus::prelude::*;

if cuda_available() {
    println!("CUDA available with {} devices", cuda_device_count());
}
```

## Basic Usage

### Rust API

```rust
use horus::prelude::*;

// Process A: Create pool and allocate tensor
let pool = CudaTensorPool::new(1, 0, CudaTensorPoolConfig::default())?;
let tensor = pool.alloc(&[1080, 1920, 3], TensorDtype::F32)?;

// Get IPC handle to share (64 bytes)
let ipc_handle = tensor.ipc_handle_bytes();

// Send ipc_handle through Hub/Link/socket to Process B...

// Process B: Open shared GPU memory
let pool = CudaTensorPool::open(1, 0)?;
let (gpu_ptr, tensor) = pool.import_ipc(ipc_handle, &[1080, 1920, 3], TensorDtype::F32)?;

// gpu_ptr points to the SAME GPU memory as Process A
// No data was copied!
```

### Python API

```python
import horus

# Check CUDA availability
if horus.cuda_is_available():
    print(f"CUDA devices: {horus.cuda_device_count()}")

# Create tensor pool
pool = horus.TensorPool(pool_id=1, size_mb=1024)

# Allocate CPU tensor and transfer to GPU
cpu_tensor = pool.alloc(shape=(1080, 1920, 3), dtype="float32")
gpu_tensor = cpu_tensor.cuda("cuda:0")

# Get IPC handle for sharing
ipc_handle = gpu_tensor.get_cuda_ipc_handle()

# Zero-copy PyTorch integration
import torch
torch_tensor = torch.as_tensor(gpu_tensor)  # Uses __cuda_array_interface__
```

## Architecture

### CudaTensorPool

The `CudaTensorPool` manages GPU memory with IPC support:

```
┌─────────────────────────────────────────────────────────────┐
│                    CudaTensorPool Design                     │
├─────────────────────────────────────────────────────────────┤
│  Process A (Owner)              Process B (Consumer)         │
│  ┌─────────────────┐            ┌─────────────────┐         │
│  │ cudaMalloc      │            │                 │         │
│  │    ↓            │            │                 │         │
│  │ GPU Memory      │════════════│ GPU Memory      │         │
│  │ (same physical) │ IPC Handle │ (same physical) │         │
│  │    ↓            │  64 bytes  │    ↓            │         │
│  │ IpcGetHandle    │───────────→│ IpcOpenHandle   │         │
│  └─────────────────┘            └─────────────────┘         │
│                                                              │
│  Shared Memory (CPU): Stores metadata + IPC handles          │
│  /dev/shm/horus/cuda_pool_{id}_{device}                     │
└─────────────────────────────────────────────────────────────┘
```

### Memory Layout

Each tensor slot stores:
- 64-byte CUDA IPC handle
- Shape, dtype, size metadata
- Atomic reference count
- Generation counter (ABA prevention)

## Integration with Hub/Link

### Sending GPU Tensors

GPU tensors integrate with HORUS Hub and Link communication:

```rust
use horus::prelude::*;

// Allocate GPU tensor
let handle = TensorHandle::alloc(pool, &[640, 480, 3], TensorDtype::F32, TensorDevice::Cuda0)?;

// Fill with data (e.g., from camera)
// ...

// Send through Hub - only 64-byte handle is copied!
hub.send(handle.tensor().clone())?;
```

### Receiving GPU Tensors

```rust
// Receiver gets tensor descriptor
let tensor = hub.recv()?;

// Wrap in handle - this opens the IPC handle if from another process
let handle = TensorHandle::from_ipc(tensor, pool)?;

// Access GPU memory directly
let gpu_ptr = handle.data_ptr();  // Points to shared GPU memory
```

## Performance Characteristics

### Latency Comparison

| Operation | Latency | Notes |
|-----------|---------|-------|
| IPC handle transfer | ~1μs | 64 bytes through shared memory |
| cudaIpcOpenMemHandle | ~10μs | One-time per handle |
| GPU memory access | 0 | Same physical memory |
| CPU→GPU copy (1080p) | ~2ms | Traditional approach |
| GPU→CPU copy (1080p) | ~2ms | Traditional approach |

### Best Use Cases

**Good for:**
- Multi-process ML inference pipelines
- Camera → Detection → Planning chains
- GPU-accelerated sensor fusion
- Real-time neural network inference

**Not ideal for:**
- Single-process applications (use TensorHandle directly)
- Small tensors (&lt;1KB) - IPC overhead dominates
- Non-Linux platforms

## Limitations

### Platform Support

| Platform | Support |
|----------|---------|
| Linux | Full support |
| Windows | Limited (significant performance penalty) |
| macOS | Not supported (no CUDA) |

### CUDA IPC Constraints

- Both processes must have same CUDA driver version
- Memory must be allocated with `cudaMalloc` (not unified memory)
- Maximum ~64 concurrent IPC handles per process (driver limit)
- Consumer must close handle before producer frees memory

## Troubleshooting

### "CUDA not available"

```bash
# Check NVIDIA driver
nvidia-smi

# Check CUDA toolkit
nvcc --version

# Ensure libcudart is installed
ls /usr/lib/x86_64-linux-gnu/libcudart*
```

### "Failed to open IPC handle"

- Verify both processes use same GPU
- Check CUDA driver versions match
- Ensure producer hasn't freed the memory

### Memory Leaks

Always close imported IPC handles:

```rust
// When done with imported tensor
pool.close_ipc(gpu_ptr)?;
```

## Example: ML Inference Pipeline

Complete example of zero-copy GPU sharing between perception and planning:

```rust
// perception_node.rs
use horus::prelude::*;

struct PerceptionNode {
    pool: CudaTensorPool,
    output: Hub<HorusTensor>,
}

impl Node for PerceptionNode {
    fn tick(&mut self, _ctx: &mut NodeInfo) {
        // Allocate GPU tensor for inference output
        let tensor = self.pool.alloc(&[100, 7], TensorDtype::F32).unwrap();

        // Run YOLO inference (fills tensor on GPU)
        self.run_inference(&tensor);

        // Send IPC handle - receiver gets same GPU memory!
        self.output.send(tensor).unwrap();
    }
}
```

```rust
// planning_node.rs
struct PlanningNode {
    pool: CudaTensorPool,
    input: Hub<HorusTensor>,
}

impl Node for PlanningNode {
    fn tick(&mut self, _ctx: &mut NodeInfo) {
        if let Some(tensor) = self.input.recv() {
            // Import GPU tensor - zero copy!
            let (gpu_ptr, _) = self.pool.import_ipc(
                &tensor.ipc_handle,
                &[100, 7],
                TensorDtype::F32
            ).unwrap();

            // Use GPU pointer directly for planning
            self.plan_path(gpu_ptr);
        }
    }
}
```

## Tensor Views and Slices

CUDA tensors support zero-copy view and slice operations for efficient memory manipulation without data movement.

### Creating Views

Reshape a tensor without copying data:

```rust
use horus::prelude::*;

let pool = CudaTensorPool::new(1, 0, Default::default())?;
let tensor = pool.alloc(&[12, 8], TensorDtype::F32)?;  // 96 elements

// Reshape to different dimensions (same total elements)
let view_3d = tensor.view(&[4, 3, 8]).unwrap();  // 96 elements
let view_1d = tensor.view(&[96]).unwrap();       // Flatten

// Views share the same GPU memory - no copy!
assert_eq!(tensor.numel, view_3d.numel);
```

### Slicing Tensors

Extract a portion of a tensor along any dimension:

```rust
// Slice along first dimension
let tensor = pool.alloc(&[100, 64, 64], TensorDtype::F32)?;  // Batch of 100

// Get items 10-20 (zero-copy slice)
let batch_slice = tensor.slice_first_dim(10, 20).unwrap();
assert_eq!(batch_slice.shape[0], 10);  // 10 items

// Slice along any dimension
let spatial_crop = tensor.slice_dim(1, 16, 48).unwrap();  // Crop dim 1
assert_eq!(spatial_crop.shape[1], 32);  // 48 - 16 = 32
```

### Dimension Manipulation

```rust
// Transpose dimensions (swaps strides, no copy)
let tensor = pool.alloc(&[3, 224, 224], TensorDtype::F32)?;  // CHW
let transposed = tensor.transpose(0, 2).unwrap();  // WHC
assert_eq!(transposed.shape, [224, 224, 3]);

// Squeeze: remove size-1 dimensions
let batched = pool.alloc(&[1, 64, 64], TensorDtype::F32)?;
let squeezed = batched.squeeze(0).unwrap();
assert_eq!(squeezed.shape[0], 64);  // Now [64, 64]

// Unsqueeze: add size-1 dimension
let unbatched = pool.alloc(&[64, 64], TensorDtype::F32)?;
let batched = unbatched.unsqueeze(0).unwrap();
assert_eq!(batched.shape[0], 1);  // Now [1, 64, 64]
```

### Checking Contiguity

Views and slices may become non-contiguous. Check before operations that require contiguous memory:

```rust
let tensor = pool.alloc(&[10, 20, 30], TensorDtype::F32)?;

// Original is always contiguous
assert!(tensor.is_contiguous());

// Slices along first dim are contiguous
let slice = tensor.slice_first_dim(0, 5).unwrap();
assert!(slice.is_contiguous());

// Transpose makes it non-contiguous
let transposed = tensor.transpose(0, 1).unwrap();
assert!(!transposed.is_contiguous());

// Views require contiguous input
assert!(transposed.view(&[6000]).is_none());  // Fails
```

## Multi-GPU Peer-to-Peer (P2P)

For systems with multiple GPUs, HORUS supports direct GPU-to-GPU memory transfers without CPU involvement.

### Checking P2P Capability

```rust
use horus::prelude::*;

// Check if GPU 0 can access GPU 1 directly
if P2PManager::can_access_peer(0, 1)? {
    println!("P2P supported between GPU 0 and GPU 1");
}

// Get full P2P topology
let topology = P2PManager::get_p2p_topology()?;
for info in topology {
    println!("GPU {} → GPU {}: {}",
        info.device, info.peer_device,
        if info.can_access { "P2P" } else { "via CPU" }
    );
}
```

### Enabling P2P Access

```rust
let p2p = P2PManager::new();

// Enable bidirectional P2P access between two GPUs
p2p.enable_bidirectional(0, 1)?;

// Or enable one-way access
p2p.enable_peer_access(0, 1)?;  // GPU 0 can access GPU 1
```

### P2P Memory Copies

```rust
use horus::prelude::*;

let p2p = P2PManager::new();
p2p.enable_bidirectional(0, 1)?;

// Create pools on different GPUs
let pool_gpu0 = CudaTensorPool::new(1, 0, Default::default())?;
let pool_gpu1 = CudaTensorPool::new(2, 1, Default::default())?;

// Allocate tensor on GPU 0
let src_tensor = pool_gpu0.alloc(&[1024, 1024], TensorDtype::F32)?;

// Direct P2P copy to GPU 1 (synchronous)
let dst_tensor = p2p.copy_p2p(&pool_gpu0, &src_tensor, &pool_gpu1)?;

// Async P2P copy with CUDA stream
let stream = cuda_ffi::stream_create()?;
let dst_tensor = pool_gpu1.alloc(&[1024, 1024], TensorDtype::F32)?;
p2p.copy_p2p_async(&pool_gpu0, &src_tensor, &pool_gpu1, &dst_tensor, stream)?;
cuda_ffi::stream_synchronize(stream)?;
```

### P2P Performance

| Interconnect | Bandwidth | Use Case |
|--------------|-----------|----------|
| PCIe 3.0 x16 | ~12 GB/s | Budget multi-GPU |
| PCIe 4.0 x16 | ~25 GB/s | Standard workstations |
| NVLink 3.0 | ~300 GB/s | High-end compute |

## CUDA Streams and Async Operations

HORUS supports asynchronous GPU operations for overlapping computation and data transfer.

### Creating Streams

```rust
use horus::prelude::*;

// Create a non-blocking stream
let stream = cuda_ffi::stream_create_with_flags(
    cuda_ffi::CudaStreamFlags::NonBlocking
)?;

// Use default stream
let default_stream = std::ptr::null_mut();
```

### Async Memory Operations

```rust
// Async device-to-host copy
let host_buffer: Vec<f32> = vec![0.0; tensor.numel as usize];
cuda_ffi::memcpy_async(
    host_buffer.as_ptr() as *mut _,
    pool.device_ptr(&tensor),
    tensor.size as usize,
    cuda_ffi::CudaMemcpyKind::DeviceToHost,
    stream,
)?;

// Continue other work while copy happens...

// Wait for completion when needed
cuda_ffi::stream_synchronize(stream)?;
```

### Events for Synchronization

```rust
// Create event
let event = cuda_ffi::event_create()?;

// Record event on stream
cuda_ffi::event_record(event, stream)?;

// Wait for event from another stream
cuda_ffi::stream_wait_event(other_stream, event)?;

// Query if event completed (non-blocking)
if cuda_ffi::event_query(event).is_ok() {
    println!("Event completed");
}
```

## Pinned (Page-Locked) Memory

For faster CPU↔GPU transfers, use pinned host memory:

```rust
use horus::prelude::*;

// Allocate pinned host memory
let size = 1024 * 1024 * 4;  // 4MB
let pinned_ptr = cuda_ffi::host_malloc(size)?;

// Register existing memory as pinned
let buffer: Vec<f32> = vec![0.0; 1_000_000];
cuda_ffi::host_register(
    buffer.as_ptr() as *mut _,
    buffer.len() * 4,
    cuda_ffi::CudaHostRegisterFlags::Portable,
)?;

// Unregister when done
cuda_ffi::host_unregister(buffer.as_ptr() as *mut _)?;
```

### Pinned Memory Benefits

| Transfer Type | Regular Memory | Pinned Memory | Speedup |
|--------------|----------------|---------------|---------|
| H→D (1GB) | ~800 MB/s | ~12 GB/s | 15x |
| D→H (1GB) | ~800 MB/s | ~12 GB/s | 15x |
| Async capable | No | Yes | ∞ |

## See Also

- [TensorPool API](/rust/api/tensor-pool) - CPU tensor management
- [Performance Benchmarks](/performance/benchmarks) - Latency measurements
- [Python Bindings](/python/api/python-bindings) - Python bindings
