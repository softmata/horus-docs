---
title: Robot Architectures
description: Visual guides for composing built-in nodes into complete robot systems
order: 15
---

# Robot Architectures

This guide shows how to compose **built-in nodes** into complete robot systems. Each architecture includes visual diagrams, topic connections, and ready-to-run code.

> **Philosophy**: HORUS built-in nodes are designed to work together. Connect them via topics to build complex systems without writing custom code.

---

## Architecture Patterns

### The Pipeline Pattern

The fundamental pattern in robotics: data flows from sensors through processing to actuators.

<MermaidDiagram
  chart={`%%{init: {'flowchart': {'padding': 15}}}%%
flowchart LR
    S["<b>Sensor</b><br/>(P: 0)"]
    F["<b>Filter</b><br/>(P: 1)"]
    C["<b>Control</b><br/>(P: 2)"]
    A["<b>Actuator</b><br/>(P: 3)"]

    S -->|"topic"| F -->|"topic"| C -->|"topic"| A
`}
  caption="The Pipeline Pattern"
/>

**Key principle**: Lower priority numbers run first. Sensors (P:0) publish data before controllers (P:2) process it.

---

## 1. Mobile Robot Base (Differential Drive)

The most common robot configuration: two-wheeled differential drive with joystick control.

### Architecture Diagram

<MermaidDiagram
  chart={`%%{init: {'flowchart': {'padding': 20}}}%%
flowchart TB
    subgraph TITLE["Mobile Robot Base System"]
        direction TB
        JOY["<b>JoystickNode</b><br/>(Priority 0)<br/>Reads gamepad/arrows"]

        T1["joystick.input"]

        DIFF["<b>DifferentialDriveNode</b><br/>(Priority 1)<br/>Converts input to wheel velocities"]

        T2["motor.left"]
        T3["motor.right"]

        LMOT["<b>DcMotorNode (Left)</b><br/>(Priority 2)<br/>GPIO PWM Pin 12, 13"]
        RMOT["<b>DcMotorNode (Right)</b><br/>(Priority 2)<br/>GPIO PWM Pin 18, 19"]

        JOY -->|"JoystickInput"| T1
        T1 --> DIFF
        DIFF -->|"f32 speed"| T2
        DIFF -->|"f32 speed"| T3
        T2 --> LMOT
        T3 --> RMOT
    end
`}
  caption="Mobile Robot Base Architecture"
/>

### Topic Connections

| From Node | Topic | Message Type | To Node |
|-----------|-------|--------------|---------|
| JoystickNode | `joystick.input` | JoystickInput | DifferentialDriveNode |
| DifferentialDriveNode | `motor.left` | f32 | DcMotorNode (left) |
| DifferentialDriveNode | `motor.right` | f32 | DcMotorNode (right) |

### Code (20 lines)

```rust
use horus::prelude::*;
use horus::prelude::*;

fn main() -> Result<()> {
    let mut scheduler = Scheduler::new();

    // Input: Joystick (Priority 0 - runs first)
    let joystick = JoystickNode::new()?;
    scheduler.add(Box::new(joystick), 0, Some(true));

    // Control: Differential drive (Priority 1)
    let diff_drive = DifferentialDriveNode::new(
        "joystick.input",   // Input topic
        "motor.left",       // Left motor output
        "motor.right",      // Right motor output
        0.3                 // Wheel separation (meters)
    )?;
    scheduler.add(Box::new(diff_drive), 1, Some(true));

    // Actuators: DC Motors (Priority 2 - run last)
    let mut left_motor = DcMotorNode::new()?;
    left_motor.configure_gpio(12, 13);  // PWM, DIR pins
    left_motor.set_input_topic("motor.left");
    scheduler.add(Box::new(left_motor), 2, Some(true));

    let mut right_motor = DcMotorNode::new()?;
    right_motor.configure_gpio(18, 19);
    right_motor.set_input_topic("motor.right");
    scheduler.add(Box::new(right_motor), 2, Some(true));

    scheduler.run()
}
```

---

## 2. Autonomous Mobile Robot (with Obstacle Avoidance)

A self-driving robot that uses LiDAR to detect obstacles and navigate safely.

### Architecture Diagram

<MermaidDiagram
  chart={`%%{init: {'flowchart': {'padding': 20}}}%%
flowchart TB
    subgraph SYSTEM["Autonomous Mobile Robot System"]
        direction TB

        subgraph SAFETY["Safety Layer (Priority 0)"]
            ESTOP["<b>EmergencyStopNode</b><br/>E-stop button, Watchdog<br/>Monitors: cmd_vel"]
        end

        subgraph SENSORS["Sensor Layer (Priority 1)"]
            LIDAR["<b>LidarNode</b><br/>360° scan, 10Hz<br/>/dev/ttyUSB0"]
        end

        T1[["lidar.scan<br/><i>LaserScan</i>"]]

        subgraph PERCEPTION["Perception Layer (Priority 2)"]
            DETECT["<b>CollisionDetectorNode</b><br/>Front arc ±45°<br/>Safety distance: 0.5m"]
        end

        T2[["obstacles<br/><i>Obstacles</i>"]]

        subgraph PLANNING["Planning Layer (Priority 3)"]
            PLAN["<b>PathPlannerNode</b><br/>VFH/DWA algorithms<br/>Obstacle avoidance"]
        end

        T3[["cmd_vel<br/><i>CmdVel</i>"]]

        subgraph CONTROL["Control Layer (Priority 4)"]
            DIFF["<b>DifferentialDriveNode</b><br/>CmdVel → wheel speeds"]
        end

        T4[["motor.left"]]
        T5[["motor.right"]]

        subgraph ACTUATORS["Actuator Layer (Priority 5)"]
            LMOT["<b>BldcMotorNode</b><br/>(Left)"]
            RMOT["<b>BldcMotorNode</b><br/>(Right)"]
        end

        LIDAR --> T1 --> DETECT
        DETECT --> T2 --> PLAN
        PLAN --> T3 --> DIFF
        T3 -.->|"monitors"| ESTOP
        DIFF --> T4 --> LMOT
        DIFF --> T5 --> RMOT
    end
`}
  caption="Autonomous Mobile Robot Architecture"
/>

### Topic Connections

| From Node | Topic | Message Type | To Node |
|-----------|-------|--------------|---------|
| LidarNode | `lidar.scan` | LaserScan | CollisionDetectorNode |
| CollisionDetectorNode | `obstacles` | Obstacles | PathPlannerNode |
| PathPlannerNode | `cmd_vel` | CmdVel | DifferentialDriveNode, EmergencyStopNode |
| DifferentialDriveNode | `motor.left` | f32 | BldcMotorNode (left) |
| DifferentialDriveNode | `motor.right` | f32 | BldcMotorNode (right) |

### Code (35 lines)

```rust
use horus::prelude::*;
use horus::prelude::*;

fn main() -> Result<()> {
    let mut scheduler = Scheduler::new();

    // SAFETY LAYER (Priority 0) - Always runs first!
    let estop = EmergencyStopNode::new("cmd_vel")?;
    scheduler.add(Box::new(estop), 0, Some(true));

    // SENSOR LAYER (Priority 1)
    let mut lidar = LidarNode::new()?;
    lidar.configure_serial("/dev/ttyUSB0", 115200);
    scheduler.add(Box::new(lidar), 1, Some(true));

    // PERCEPTION LAYER (Priority 2)
    let mut detector = CollisionDetectorNode::new()?;
    detector.set_input_topic("lidar.scan");
    detector.set_output_topic("obstacles");
    detector.set_safety_distance(0.5);  // 50cm
    scheduler.add(Box::new(detector), 2, Some(true));

    // PLANNING LAYER (Priority 3)
    let mut planner = PathPlannerNode::new()?;
    planner.set_obstacle_topic("obstacles");
    planner.set_output_topic("cmd_vel");
    scheduler.add(Box::new(planner), 3, Some(true));

    // CONTROL LAYER (Priority 4)
    let diff_drive = DifferentialDriveNode::new(
        "cmd_vel", "motor.left", "motor.right", 0.3
    )?;
    scheduler.add(Box::new(diff_drive), 4, Some(true));

    // ACTUATOR LAYER (Priority 5)
    let mut left_motor = BldcMotorNode::new()?;
    left_motor.configure_gpio(12, EscProtocol::DShot600);
    left_motor.set_input_topic("motor.left");
    scheduler.add(Box::new(left_motor), 5, Some(true));

    let mut right_motor = BldcMotorNode::new()?;
    right_motor.configure_gpio(13, EscProtocol::DShot600);
    right_motor.set_input_topic("motor.right");
    scheduler.add(Box::new(right_motor), 5, Some(true));

    scheduler.run()
}
```

---

## 3. Sensor Fusion System (LiDAR + IMU + Odometry)

Combine multiple sensors for accurate robot localization.

### Architecture Diagram

<MermaidDiagram
  chart={`%%{init: {'flowchart': {'padding': 20}}}%%
flowchart TB
    subgraph SYSTEM["Sensor Fusion System"]
        direction TB

        subgraph SENSORS["Sensor Layer (Priority 1 - parallel)"]
            direction LR
            LIDAR["<b>LidarNode</b><br/>2D scan, 10Hz"]
            IMU["<b>ImuNode</b><br/>Accel/Gyro, 100Hz"]
            ODOM["<b>OdometryNode</b><br/>Wheel encoders, 50Hz"]
        end

        T1[["lidar.scan"]]
        T2[["imu.data"]]
        T3[["odom.twist"]]

        subgraph FUSION["Fusion Layer (Priority 2)"]
            LOC["<b>LocalizationNode</b><br/>Extended Kalman Filter (EKF)<br/>State: [x, y, θ, vx, vy, ω]<br/>Update: 100Hz"]
        end

        T4[["robot.pose<br/><i>Pose2D</i>"]]

        subgraph CONSUMERS["Consumers (Priority 3+)"]
            direction LR
            PLAN["<b>PathPlannerNode</b><br/>Uses pose for planning"]
            MON["<b>Monitor/Logger</b><br/>Visualization"]
        end

        LIDAR --> T1
        IMU --> T2
        ODOM --> T3
        T1 --> LOC
        T2 --> LOC
        T3 --> LOC
        LOC --> T4
        T4 --> PLAN
        T4 --> MON
    end
`}
  caption="Sensor Fusion Architecture"
/>

### Topic Connections

| From Node | Topic | Message Type | To Node |
|-----------|-------|--------------|---------|
| LidarNode | `lidar.scan` | LaserScan | LocalizationNode |
| ImuNode | `imu.data` | ImuData | LocalizationNode |
| OdometryNode | `odom.twist` | Twist | LocalizationNode |
| LocalizationNode | `robot.pose` | Pose2D | PathPlannerNode, Monitor |

### Code (25 lines)

```rust
use horus::prelude::*;
use horus::prelude::*;

fn main() -> Result<()> {
    let mut scheduler = Scheduler::new();

    // SENSORS (Priority 1 - parallel execution)
    let mut lidar = LidarNode::new()?;
    lidar.set_output_topic("lidar.scan");
    scheduler.add(Box::new(lidar), 1, Some(true));

    let mut imu = ImuNode::new()?;
    imu.configure_i2c("/dev/i2c-1", 0x68);  // MPU6050
    imu.set_output_topic("imu.data");
    scheduler.add(Box::new(imu), 1, Some(true));

    let mut odom = OdometryNode::new()?;
    odom.configure_encoders(17, 18, 27, 22);  // Left A/B, Right A/B
    odom.set_output_topic("odom.twist");
    scheduler.add(Box::new(odom), 1, Some(true));

    // FUSION (Priority 2)
    let mut localization = LocalizationNode::new()?;
    localization.subscribe_topics(&["lidar.scan", "imu.data", "odom.twist"]);
    localization.set_output_topic("robot.pose");
    scheduler.add(Box::new(localization), 2, Some(true));

    scheduler.run()
}
```

---

## 4. Vision-Based Robot (Camera + AI Detection)

A robot that uses computer vision for object detection and tracking.

### Architecture Diagram

<MermaidDiagram
  chart={`%%{init: {'flowchart': {'padding': 20}}}%%
flowchart TB
    subgraph SYSTEM["Vision-Based Robot System"]
        direction TB

        subgraph SAFETY["Safety Layer (Priority 0)"]
            SAFE["<b>SafetyMonitorNode</b><br/>Detects 'person' class<br/>Triggers slow/stop if close"]
        end

        subgraph SENSOR["Sensor Layer (Priority 1)"]
            CAM["<b>CameraNode</b><br/>USB/CSI, 30 FPS<br/>640×480 RGB"]
        end

        T1[["camera.image<br/><i>Image</i>"]]

        subgraph PERCEPTION["Perception Layer (Priority 2)"]
            YOLO["<b>YOLOv8DetectorNode</b><br/>80 COCO classes<br/>GPU: CUDA/TensorRT<br/>~30ms inference"]
        end

        T2[["detections<br/><i>Detections</i>"]]

        subgraph TRACKING["Tracking Layer (Priority 3)"]
            direction LR
            TRACK["<b>ObjectTrackerNode</b><br/>Assigns unique IDs<br/>Predicts positions"]
        end

        T3[["tracked_objects"]]

        subgraph BEHAVIOR["Behavior Layer (Priority 4)"]
            FOLLOW["<b>FollowBehaviorNode</b><br/>Follows target object<br/>Maintains safe distance"]
        end

        T4[["cmd_vel<br/><i>CmdVel</i>"]]

        subgraph CONTROL["Control Layer (Priority 5)"]
            DIFF["<b>DifferentialDriveNode</b> → Motors"]
        end

        CAM --> T1 --> YOLO
        YOLO --> T2
        T2 --> TRACK
        T2 -.->|"monitors"| SAFE
        TRACK --> T3 --> FOLLOW
        FOLLOW --> T4 --> DIFF
        T4 -.->|"monitors"| SAFE
    end
`}
  caption="Vision-Based Robot Architecture"
/>

### Topic Connections

| From Node | Topic | Message Type | To Node |
|-----------|-------|--------------|---------|
| CameraNode | `camera.image` | Image | YOLOv8DetectorNode |
| YOLOv8DetectorNode | `detections` | Detections | ObjectTrackerNode, SafetyMonitorNode |
| ObjectTrackerNode | `tracked_objects` | TrackedObjects | FollowBehaviorNode |
| FollowBehaviorNode | `cmd_vel` | CmdVel | DifferentialDriveNode |

### Code (30 lines)

```rust
use horus::prelude::*;
use horus::prelude::*;

fn main() -> Result<()> {
    let mut scheduler = Scheduler::new();

    // SAFETY (Priority 0)
    let safety = SafetyMonitorNode::new()?;
    scheduler.add(Box::new(safety), 0, Some(true));

    // SENSOR (Priority 1)
    let mut camera = CameraNode::new()?;
    camera.configure_device("/dev/video0");
    camera.set_resolution(640, 480);
    camera.set_fps(30);
    camera.set_output_topic("camera.image");
    scheduler.add(Box::new(camera), 1, Some(true));

    // PERCEPTION (Priority 2)
    let mut detector = YOLOv8DetectorNode::new()?;
    detector.load_model("yolov8n.onnx");  // Nano model for speed
    detector.set_input_topic("camera.image");
    detector.set_output_topic("detections");
    detector.set_confidence_threshold(0.5);
    scheduler.add(Box::new(detector), 2, Some(true));

    // CONTROL (Priority 4)
    let diff_drive = DifferentialDriveNode::new(
        "cmd_vel", "motor.left", "motor.right", 0.3
    )?;
    scheduler.add(Box::new(diff_drive), 4, Some(true));

    // ACTUATORS (Priority 5)
    // ... motor nodes ...

    scheduler.run()
}
```

---

## 5. Industrial Robot Arm (Multi-Joint Servo Control)

A robot arm with multiple Dynamixel servos for precise manipulation.

### Architecture Diagram

<MermaidDiagram
  chart={`%%{init: {'flowchart': {'padding': 20}}}%%
flowchart TB
    subgraph SYSTEM["Industrial Robot Arm System"]
        direction TB

        subgraph PLANNING["Planning Layer (Priority 1)"]
            TRAJ["<b>TrajectoryNode</b><br/>Smooth path generation<br/>Interpolation"]
        end

        T1[["arm.trajectory<br/><i>JointTrajectory</i>"]]

        subgraph CONTROL["Control Layer (Priority 2)"]
            DYN["<b>DynamixelNode</b><br/>6× XM430-W350 servos<br/>Position/velocity/torque modes<br/>Protocol 2.0, 1Mbps"]
        end

        T2[["arm.joint_states<br/><i>JointState</i>"]]

        subgraph FEEDBACK["Feedback Layer (Priority 3)"]
            direction LR
            FK["<b>ForwardKinematicsNode</b><br/>End-effector pose<br/>DH parameters"]
            COL["<b>CollisionCheckNode</b><br/>Self-collision detection<br/>Workspace limits"]
        end

        T3[["arm.end_effector_pose<br/><i>Pose3D</i>"]]

        subgraph EFFECTOR["End Effector (Priority 4)"]
            GRIP["<b>GripperNode</b><br/>Open/close control<br/>Force feedback"]
        end

        TRAJ --> T1 --> DYN
        DYN --> T2
        T2 --> FK
        T2 --> COL
        FK --> T3 --> GRIP
    end
`}
  caption="Industrial Robot Arm Architecture"
/>

### Topic Connections

| From Node | Topic | Message Type | To Node |
|-----------|-------|--------------|---------|
| TrajectoryNode | `arm.trajectory` | JointTrajectory | DynamixelNode |
| DynamixelNode | `arm.joint_states` | JointState | ForwardKinematicsNode, CollisionCheckNode |
| ForwardKinematicsNode | `arm.end_effector_pose` | Pose3D | Application layer |

---

## Priority Assignment Guidelines

### Standard Priority Layers

| Priority | Layer | Purpose | Example Nodes |
|----------|-------|---------|---------------|
| 0 | **Safety** | Emergency stop, watchdogs | EmergencyStopNode, SafetyMonitorNode |
| 1 | **Sensors** | Raw data acquisition | LidarNode, CameraNode, ImuNode |
| 2 | **Perception** | Data processing, detection | YOLOv8DetectorNode, CollisionDetectorNode |
| 3 | **Planning** | Decision making, path planning | PathPlannerNode, LocalizationNode |
| 4 | **Control** | Velocity/force commands | DifferentialDriveNode, PidControllerNode |
| 5 | **Actuators** | Hardware output | DcMotorNode, BldcMotorNode, ServoNode |
| 6+ | **Logging** | Non-critical monitoring | DataLoggerNode, Monitor |

### Why This Order?

1. **Safety first**: Emergency stop can override everything
2. **Sensors before perception**: Need fresh data to process
3. **Perception before planning**: Need to know obstacles before planning
4. **Planning before control**: Need path before generating velocities
5. **Control before actuators**: Need commands before moving motors

---

## Default Topic Names (Built-in Nodes)

For interoperability, built-in nodes use these default topics:

| Category | Topic | Message Type | Used By |
|----------|-------|--------------|---------|
| **Input** | `joystick/input` | JoystickInput | JoystickNode |
| **Input** | `keyboard/input` | KeyboardInput | KeyboardInputNode |
| **Sensors** | `lidar.scan` | LaserScan | LidarNode |
| **Sensors** | `camera.image` | Image | CameraNode |
| **Sensors** | `imu.data` | ImuData | ImuNode |
| **Sensors** | `gps/fix` | NavSatFix | GpsNode |
| **Sensors** | `ultrasonic/range` | Range | UltrasonicNode |
| **Perception** | `detections` | Detections | YOLOv8DetectorNode |
| **Perception** | `obstacles` | Obstacles | CollisionDetectorNode |
| **Localization** | `robot.pose` | Pose2D | LocalizationNode |
| **Localization** | `odom.twist` | Twist | OdometryNode |
| **Control** | `cmd_vel` | CmdVel | Most control nodes |
| **Motors** | `motor.left` | f32 | DifferentialDriveNode |
| **Motors** | `motor.right` | f32 | DifferentialDriveNode |

---

## Next Steps

- **[Built-in Nodes Index](/rust/library/built-in-nodes)** - Complete list of 38 production-ready nodes
- **[Using Pre-Built Nodes](/package-management/using-prebuilt-nodes)** - Installation and configuration
- **[Basic Examples](/rust/examples/basic-examples)** - Runnable code examples
- **[Hub Communication](/concepts/core-concepts-hub)** - How pub/sub works
